TASK 1: 

1️⃣ Python Clarity:

What i focues on:

With context to project I routed myself to just focusing on python in context to files, pdfs not concepts like OOP, Class or complex data structres.

What i did:

scripts/read-files.py contains basic file handling stuff which is done by using data/sample.txt as a data.
What have i learnt:
A quick refresh to my file handling knowledge
Refresh to string handling and regex
Learnt the usage of shutil
Learnt about unicode escape issues 
Referred mostly from GeeksforGeeks for help and task is generated by ChatGPT


TASK 2:

2️⃣ Data & Metadata Basics:

What i focused on:

I focused on understanding what kind of data that we were going to handle and how to metadata those.

What i did:

I read the mentioned atricles(https://www.coursera.org/articles/what-is-metadata, https://www.tessresearch.org/scientific-paper-part-1/, https://www.tessresearch.org/scientific-paper-part-2/) and got few ideas using AI too. 

What i learnt:

Metadata is “data about data” and helps describe what a document is and how to interpret it.
A DOI acts like a permanent, unique identifier for a paper, similar to a digital fingerprint.
Some metadata fields reveal meaning faster — especially title, abstract, and keywords — because they summarize intent before reading the full document.
Created a metadata on the data/sample.pdf and it is present in outputs/manual_metadata.json


TASK3:

3️⃣ Working with PDF & Word Files:

What i foucsed on:

I focused on extracting data from PDFs and Word files, I've used PyMuPDF for PDF extarction and python - docx for extarcting data from word documents.

What i did:

I used the same document in PDF and DOCX to compare extraction behavior
data/realistic_extraction_paper.pdf  
data/realistic_extraction_paper.docx

Splitting experiments I tried:

Basic blank-line splitter:
Did not work well because PDFs often have single newlines, not blank lines.

Inserted blank lines before numbered headings:
Improved segmentation a little.

Inserted blank lines before known headings:
Helped but still not perfect.

Inserted single \n before inline headings:
Broke paragraphs into uneven chunks.

Collapsed internal line-wrap newlines into spaces:
Fixed mid-sentence breaks.

Root cause: inserting single `\n` before inline headings is not sufficient for the `\n\n+` split step — single newlines remained and collapsing later joined fragments in awkward places.

What i learnt:

PyMuPDF for reading PDF text page-by-page
python-docx for reading document paragraphs
spli() - splits whatever escape sequences given in it
strip() - removes spaces at the beginning and end
enumerate() - we can iterate 2 things at a time
json.load() / json.dump() → work with JSON files
re.sub() → edit text using regex
re.split() → split using regex patterns

Why PDF extraction is tricky:

PDF paragraphs do NOT come in well-structured blocks
Line breaks do not equal real paragraphs
Must create artificial boundaries before headings and known section names

I now have a certain amount of understanding on regex but have to work more in that!


TASK 4: 

What I focused on:

Combining all extracted components (paragraphs, headings, captions, references, links, manifest values)
Learning how to organize outputs into a single structured JSON
Designing a proper metadata schema used in real extraction systems
Creating a new script (build_metadata.py) to assemble final metadata files for PDF and DOCX

What I Did:

Loaded all intermediate JSON files generated earlier:

*_paragraphs.json, *_headings.json, *_captions.json, *_references.json, *_reference_links.json, *_manifest.json, *_content.txt

Computed:

word_count,page_count (for PDF),caption counts,heading counts,reference counts

Created one unified metadata dictionary for each document

Saved final outputs:
outputs/pdf_metadata.json
outputs/docx_metadata.json

What I Learned:

How to assemble multi-stage extraction outputs
How real metadata pipelines work
How to load multiple JSON files safely
How to calculate useful document statistics
How to design a clean metadata schema